{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel BFS performance profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note on formatting: the conversion from python notebook to latex to pdf rendered some issues in image placement. The label on the image should clarify to which entry it belongs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entry 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SBATCH  -J bfs                          # Job name\n",
    "#SBATCH  -p development                  # Queue (development or normal)\n",
    "#SBATCH  -N 1                            # Number of nodes\n",
    "#SBATCH --tasks-per-node 2               # Number of tasks per node\n",
    "#SBATCH  -t 00:10:00                     # Time limit hrs:min:sec\n",
    "#SBATCH  -A TG-TRA170035                 # Allocation\n",
    "#SBATCH  -o bfs=%j.out                   # Standard output and error log\n",
    "\n",
    "module use /home1/01236/tisaac/opt/modulefiles\n",
    "module load petsc/cse6230-double\n",
    "\n",
    "make test_bfs\n",
    "\n",
    "git rev-parse HEAD\n",
    "\n",
    "git diff-files\n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "#ibrun tacc_affinity ./test_bfs\n",
    "ibrun tacc_affinity hpcrun -t ./test_bfs -tests 4\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The following have been reloaded with a version change:\n",
    "  1) petsc/cse6230-single => petsc/cse6230-double\n",
    "\n",
    "mpicc -o test_bfs.o -c -fPIC  -wd1572 -g  -g -I/home1/01236/tisaac/\n",
    "opt/petsc/cse6230-double/include \n",
    "-I/home1/01236/tisaac/opt/petsc/cse6230-double/include -I\n",
    "/home1/01236/tisaac/opt/petsc/cse6230-double/include\n",
    "`pwd`/test_bfs.c\n",
    "gmake[1]: Nothing to be done for `libc'.\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "The version of PETSc you are using is out-of-date, \n",
    "we recommend updating to the new release\n",
    " Available Version: 3.8.3   Installed Version: 3.8.1\n",
    "http://www.mcs.anl.gov/petsc/download/index.html\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "mpicc -fPIC  -wd1572 -g -g -o test_bfs test_bfs.o \n",
    "true test_bfs\n",
    "/bin/rm -f -f test_bfs.o\n",
    "2348ab557cd6754dd64f26e8716c813ebac5d551\n",
    "/home1/05267/tg845668/cse6230fa17-final-kkc3/Breadth-First-Search/parallel\n",
    "c455-061.stampede2.tacc.utexas.edu\n",
    "Wed Dec 13 01:45:46 CST 2017\n",
    "TACC:  Starting up job 508540\n",
    "TACC:  Starting parallel tasks...\n",
    "Running 1 tests of breadth first search\n",
    "  Test 0: scale 4\n",
    "    Test: 256 edges\n",
    "    Test scale 4, 15 keys: Passed, average time (10 tests): 0.0103887, \n",
    "            ***23102.1 parents per second***.\n",
    "===Harmonic mean: 23102.1 parents per second===\n",
    "TACC:  Shutdown complete. Exiting.\n",
    "Wed Dec 13 01:48:59 CST 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Entry 1: hpcviewer](entries/entry1.png \"Entry 1: hpcviewer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this first round of profiling using `hpcviewer`, I notice loop redundancies at line 454 and 460 when allocating and initializing `local_adj` and `local_adj_parents`. While I do not expect a significant performance boost from merging the two loops given their respective CPU time, it is useful to find and alleviate code inefficiencies so their impact on wall clock time does increase with a larger problem size or number of processes (in this case, the latter). The proposed change is thus to initialize the two arrays in the same loop in which their memory is allocated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entry 1b\n",
    "The same submission script is run with the proposed changes with the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpicc -o test_bfs.o -c -fPIC  -wd1572 -g  -g -I/home1/01236/tisaac/opt\n",
    "/petsc/cse6230-double/include -I/home1/01236/tisaac/opt/petsc/cse6230-double/\n",
    "include -I/home1/01236/tisaac/opt/petsc/cse6230-double/include    \n",
    "`pwd`/test_bfs.c\n",
    "gmake[1]: Nothing to be done for `libc'.\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "The version of PETSc you are using is out-of-date, \n",
    "we recommend updating to the new release\n",
    " Available Version: 3.8.3   Installed Version: 3.8.1\n",
    "http://www.mcs.anl.gov/petsc/download/index.html\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "mpicc -fPIC  -wd1572 -g -g -o test_bfs test_bfs.o\n",
    "true test_bfs\n",
    "/bin/rm -f -f test_bfs.o\n",
    "186da4dc43b1791fe729d8f56cfc3ad962fc104f\n",
    "/home1/05267/tg845668/cse6230fa17-final-kkc3/Breadth-First-Search/parallel\n",
    "c455-031.stampede2.tacc.utexas.edu\n",
    "Wed Dec 13 17:33:15 CST 2017\n",
    "TACC:  Starting up job 511153\n",
    "TACC:  Starting parallel tasks...\n",
    "Running 1 tests of breadth first search\n",
    "  Test 0: scale 4\n",
    "    Test: 256 edges\n",
    "    Test scale 4, 15 keys: Passed, average time (10 tests): 0.0122555,\n",
    "            ***19583.1 parents per second***.\n",
    "===Harmonic mean: 19583.1 parents per second===\n",
    "TACC:  Shutdown complete. Exiting.\n",
    "Wed Dec 13 17:36:32 CST 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under commit `9736ce218fe6d03a0d17ac2dcf4e5cfbde2ab8a6`, the wall clock time is tested locally by using `MPI_Wtime()` and executing with the command `mpiexec -n 2 ./test_bfs -tests 4` with the following result.\n",
    "`Average loop time: 0.000005`\n",
    "\n",
    "Under commit `04719d18155140b4b7ae062e71f545115cb52991`, the wall clock time is tested locally by running with the command `mpiexec -n 2 ./test_bfs -tests 4` to measure the average time spent in the loop after changes were applied.\n",
    "`Average loop time: 0.000005`.\n",
    "\n",
    "Although the results do not clearly support the claim that one method is faster than the other, it is also possible that the MPI wall clock time measure does not provide sufficient granularity for a meaningful comparison. It could also be the case that the loop ove the number of processes is not big enough to make a large impact when it is done twice instead of once. Had it been a loop over the number of vertices in the frontier, for example, the improvement in speed might have been more noticeable. Although the harmonic mean decreased from the first run to the second, I believe this to be due to the natural variance in the problem as opposed to a direct cause of the changes. Given more time, I would rerun this comparison on a larger problem size and number of checks to be more thorough in the comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entry 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this entry, the latest version of the code is run with `MPI_Wtime()` around the MPI all-to-all communication routines in `BFSGraphSearch`. The first routine is the `MPI_Allreduce` in charge of checking whether all processes are done with their searches. The second routine timed is `MPI_Alltoall` that transposes the number of adjacencies in each process. The second routine is an `MPI_Alltoallv` (denoted by Alltoallv 1 in the output) which exchanges the adjacencies to their respective owner process, and the final `MPI_Alltoallv` (denoted by Alltoallv 2) does the same but for the parents of these adjacencies. Since the third and fourth transpose operations are communicating the same number of vertices to the same processes, I would expect their timings to be very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this run, I increase the number of processes to 4 in order to gain a better perspective of work distribution and potential inefficiencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SBATCH  -J bfs                          # Job name\n",
    "#SBATCH  -p development                  # Queue (development or normal)\n",
    "#SBATCH  -N 1                            # Number of nodes\n",
    "#SBATCH --tasks-per-node 4               # Number of tasks per node\n",
    "#SBATCH  -t 00:10:00                     # Time limit hrs:min:sec\n",
    "#SBATCH  -A TG-TRA170035                 # Allocation\n",
    "#SBATCH  -o bfs=%j.out                   # Standard output and error log\n",
    "\n",
    "module use /home1/01236/tisaac/opt/modulefiles\n",
    "module load petsc/cse6230-double\n",
    "\n",
    "make test_bfs\n",
    "\n",
    "git rev-parse HEAD\n",
    "\n",
    "git diff-files\n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "#ibrun tacc_affinity ./test_bfs\n",
    "ibrun tacc_affinity hpcrun -t ./test_bfs -tests 4 -num_time 1\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpicc -o test_bfs.o -c -fPIC  -wd1572 -g  -g -I/home1/01236/tisaac/opt/petsc/cse6230-double/include -I/home1/01236/tisaac/opt/petsc/cse6230-double/include -I/home1/01236/tisaac/opt/petsc/cse6230-double/include    `pwd`/test_bfs.c\n",
    "gmake[1]: Nothing to be done for `libc'.\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "The version of PETSc you are using is out-of-date, we recommend updating to the new release\n",
    " Available Version: 3.8.3   Installed Version: 3.8.1\n",
    "http://www.mcs.anl.gov/petsc/download/index.html\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "mpicc -fPIC  -wd1572 -g -g -o test_bfs test_bfs.o\n",
    "true test_bfs\n",
    "/bin/rm -f -f test_bfs.o\n",
    "b81e96323cfb9439581b432448762bfcb66049c2\n",
    "/home1/05267/tg845668/cse6230fa17-final-kkc3/Breadth-First-Search/parallel\n",
    "c455-002.stampede2.tacc.utexas.edu\n",
    "Thu Dec 14 00:10:50 CST 2017\n",
    "TACC:  Starting up job 511636\n",
    "TACC:  Starting parallel tasks...\n",
    "Running 1 tests of breadth first search\n",
    "  Test 0: scale 4\n",
    "    Test: 256 edges\n",
    "rank: 0\n",
    "Allreduce avg time:   0.000074\n",
    "Alltoall avg time:    0.000031\n",
    "Alltoallv 1 avg time: 0.000158\n",
    "Alltoallv 2 avg time: 0.000008\n",
    "rank: 1\n",
    "Allreduce avg time:   0.000077\n",
    "Alltoall avg time:    0.000159\n",
    "Alltoallv 1 avg time: 0.000013\n",
    "Alltoallv 2 avg time: 0.000008\n",
    "rank: 2\n",
    "Allreduce avg time:   0.000063\n",
    "Alltoall avg time:    0.000161\n",
    "Alltoallv 1 avg time: 0.000013\n",
    "Alltoallv 2 avg time: 0.000008\n",
    "rank: 3\n",
    "Allreduce avg time:   0.000051\n",
    "Alltoall avg time:    0.000014\n",
    "Alltoallv 1 avg time: 0.000157\n",
    "Alltoallv 2 avg time: 0.000008\n",
    "rank: 0\n",
    "Allreduce avg time:   0.000007\n",
    "Alltoall avg time:    0.000165\n",
    "Alltoallv 1 avg time: 0.000012\n",
    "Alltoallv 2 avg time: 0.000009\n",
    "rank: 1\n",
    "Allreduce avg time:   0.000133\n",
    "Alltoall avg time:    0.000013\n",
    "Alltoallv 1 avg time: 0.000025\n",
    "Alltoallv 2 avg time: 0.000009\n",
    "rank: 2\n",
    "Allreduce avg time:   0.000011\n",
    "Alltoall avg time:    0.000136\n",
    "Alltoallv 1 avg time: 0.000025\n",
    "Alltoallv 2 avg time: 0.000009\n",
    "rank: 3\n",
    "Allreduce avg time:   0.000012\n",
    "Alltoall avg time:    0.000149\n",
    "Alltoallv 1 avg time: 0.000012\n",
    "Alltoallv 2 avg time: 0.000008\n",
    "    Test scale 4, 16 keys: Passed, average time \n",
    "    (1 tests): 0.0137491, ***18619.4 parents per second***.\n",
    "===Harmonic mean: 18619.4 parents per second===\n",
    "TACC:  Shutdown complete. Exiting.\n",
    "Thu Dec 14 00:14:11 CST 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Entry 2: hpctraceviewer](entries/entry2.png \"Entry 2: hpctraceviewer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `hpctraceviewer` analysis, I notice the process discrepancies of time spent in `MPI_Allreduce`. Given that this routine only communicates the value of a boolean flag (indicating whether the process is done) and reduces it with a logical and operator, I would expect this routine to be equally fast across processes. The viewer shows large variance, however. `MPI_Allreduce` is a blocking communication routine, meaning that each process will have to wait for the other processes to reach this point in order to execute. If some processes have less vertices to process, it could well be the case that they finish their search earlier, and are forced to wait for processes that are still working. To alleviate some of the waiting imabalance, I propose altering the code to implement a non-blocking routine so that processes that arrive at the reduction earlier can continue doing work instead of wasting time waiting. This can be achieved by using `MPI_Iallreduce`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entry 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up development and testing time, I run the original and the updated BFS locally to compare the performance of the proposed change using the non-blocking routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make test_bfs\n",
    "\n",
    "git rev-parse HEAD\n",
    "\n",
    "git diff-files\n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "mpiexec -n 4 ./test_bfs -tests 4 -num_time 1\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output (blocking reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpicc -o test_bfs.o -c -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -Qunused-arguments -fvisibility=hidden -g3  -g -I/Users/Karel/cs/cse6230/petsc/include -I/Users/Karel/cs/cse6230/petsc/arch-darwin-c-debug/include\n",
    "`pwd`/test_bfs.c\n",
    "/usr/bin/ar cr libbfs.a bfs.o\n",
    "if test -n \"\"; then /usr/bin/ar cr  bfs.lo; fi\n",
    "/bin/rm -f bfs.o bfs.lo\n",
    "mpicc -Wl,-multiply_defined,suppress -Wl,-multiply_defined -Wl,suppress -Wl,-commons,use_dylibs -Wl,-search_paths_first -Wl,-no_compact_unwind    -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -Qunused-arguments -fvisibility=hidden -g3 -g -o test_bfs test_bfs.o -Wl,-rpath,/Users/Karel/cs/cse6230/petsc/arch-darwin-c-debug/lib -L/Users/Karel/cs/cse6230/petsc/arch-darwin-c-debug/lib -Wl,-rpath,/usr/local/opt/libevent/lib -L/usr/local/opt/libevent/lib -Wl,-rpath,/usr/local/Cellar/open-mpi/2.1.1/lib -L/usr/local/Cellar/open-mpi/2.1.1/lib -Wl,-rpath,/Library/Developer/CommandLineTools/usr/lib/clang/8.0.0/lib/darwin -L/Library/Developer/CommandLineTools/usr/lib/clang/8.0.0/lib/darwin -Wl,-rpath,/usr/local/Cellar/gcc/7.2.0/lib/gcc/7/gcc/x86_64-apple-darwin15.6.0/7.2.0 -L/usr/local/Cellar/gcc/7.2.0/lib/gcc/7/gcc/x86_64-apple-darwin15.6.0/7.2.0 -Wl,-rpath,/usr/local/Cellar/gcc/7.2.0/lib/gcc/7 -L/usr/local/Cellar/gcc/7.2.0/lib/gcc/7 -Wl,-rpath,/Library/Developer/CommandLineTools/usr/bin/../lib/clang/8.0.0/lib/darwin -L/Library/Developer/CommandLineTools/usr/bin/../lib/clang/8.0.0/lib/darwin -lpetsc -llapack -lblas -lclang_rt.osx -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lgcc_ext.10.5 -lquadmath -lm -lclang_rt.osx -lc++ -lclang_rt.osx -Wl,-rpath,/usr/local/opt/libevent/lib -L/usr/local/opt/libevent/lib -Wl,-rpath,/usr/local/Cellar/open-mpi/2.1.1/lib -L/usr/local/Cellar/open-mpi/2.1.1/lib -ldl -lmpi -lSystem -Wl,-rpath,/Library/Developer/CommandLineTools/usr/bin/../lib/clang/8.0.0/lib/darwin -L/Library/Developer/CommandLineTools/usr/bin/../lib/clang/8.0.0/lib/darwin -lclang_rt.osx -ldl  libbfs.a\n",
    "/usr/bin/dsymutil test_bfs\n",
    "warning: (x86_64) /Users/Karel/cs/cse6230/final/cse6230fa17-final-kkc3/Breadth-First-Search/parallel/libbfs.a(bfs.o) unable to open object file: No such file or directory\n",
    "/bin/rm -f -f test_bfs.o\n",
    "18aafdeefbe473b14e0d69fe467079139ac050ac\n",
    "/Users/Karel/cs/cse6230/final/cse6230fa17-final-kkc3/Breadth-First-Search/parallel\n",
    "compette\n",
    "Thu Dec 14 04:07:23 EST 2017\n",
    "Running 1 tests of breadth first search\n",
    "  Test 0: scale 4\n",
    "    Test: 256 edges\n",
    "rank: 0\n",
    "Allreduce avg time:   0.000027\n",
    "Alltoall avg time:    0.000036\n",
    "Alltoallv 1 avg time: 0.000021\n",
    "Alltoallv 2 avg time: 0.000021\n",
    "rank: 1\n",
    "Allreduce avg time:   0.000028\n",
    "Alltoall avg time:    0.000027\n",
    "Alltoallv 1 avg time: 0.000022\n",
    "Alltoallv 2 avg time: 0.000021\n",
    "rank: 2\n",
    "Allreduce avg time:   0.000027\n",
    "Alltoall avg time:    0.000027\n",
    "Alltoallv 1 avg time: 0.000025\n",
    "Alltoallv 2 avg time: 0.000021\n",
    "rank: 3\n",
    "Allreduce avg time:   0.000032\n",
    "Alltoall avg time:    0.000024\n",
    "Alltoallv 1 avg time: 0.000022\n",
    "Alltoallv 2 avg time: 0.000024\n",
    "rank: 0\n",
    "Allreduce avg time:   0.000043\n",
    "Alltoall avg time:    0.000040\n",
    "Alltoallv 1 avg time: 0.000027\n",
    "Alltoallv 2 avg time: 0.000026\n",
    "rank: 1\n",
    "Allreduce avg time:   0.000038\n",
    "Alltoall avg time:    0.000043\n",
    "Alltoallv 1 avg time: 0.000025\n",
    "Alltoallv 2 avg time: 0.000024\n",
    "rank: 2\n",
    "Allreduce avg time:   0.000036\n",
    "Alltoall avg time:    0.000044\n",
    "Alltoallv 1 avg time: 0.000025\n",
    "Alltoallv 2 avg time: 0.000025\n",
    "rank: 3\n",
    "Allreduce avg time:   0.000031\n",
    "Alltoall avg time:    0.000039\n",
    "Alltoallv 1 avg time: 0.000035\n",
    "Alltoallv 2 avg time: 0.000025\n",
    "    Test scale 4, 16 keys: Passed, average time \n",
    "    (1 tests): 0.00950313, ***26938.5 parents per second***.\n",
    "===Harmonic mean: 26938.5 parents per second===\n",
    "Thu Dec 14 04:07:26 EST 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output (non-blocking reduction)\n",
    "In this output, `Allreduce avg time` represents the average time spent in the `Iallreduce` communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpicc -o test_bfs.o -c -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -Qunused-arguments -fvisibility=hidden -g3  -g -I/Users/Karel/cs/cse6230/petsc/include -I/Users/Karel/cs/cse6230/petsc/arch-darwin-c-debug/include    `pwd`/test_bfs.c\n",
    "make[1]: Nothing to be done for `libc'.\n",
    "mpicc -Wl,-multiply_defined,suppress -Wl,-multiply_defined -Wl,suppress -Wl,-commons,use_dylibs -Wl,-search_paths_first -Wl,-no_compact_unwind    -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -Qunused-arguments -fvisibility=hidden -g3 -g -o test_bfs test_bfs.o -Wl,-rpath,/Users/Karel/cs/cse6230/petsc/arch-darwin-c-debug/lib -L/Users/Karel/cs/cse6230/petsc/arch-darwin-c-debug/lib -Wl,-rpath,/usr/local/opt/libevent/lib -L/usr/local/opt/libevent/lib -Wl,-rpath,/usr/local/Cellar/open-mpi/2.1.1/lib -L/usr/local/Cellar/open-mpi/2.1.1/lib -Wl,-rpath,/Library/Developer/CommandLineTools/usr/lib/clang/8.0.0/lib/darwin -L/Library/Developer/CommandLineTools/usr/lib/clang/8.0.0/lib/darwin -Wl,-rpath,/usr/local/Cellar/gcc/7.2.0/lib/gcc/7/gcc/x86_64-apple-darwin15.6.0/7.2.0 -L/usr/local/Cellar/gcc/7.2.0/lib/gcc/7/gcc/x86_64-apple-darwin15.6.0/7.2.0 -Wl,-rpath,/usr/local/Cellar/gcc/7.2.0/lib/gcc/7 -L/usr/local/Cellar/gcc/7.2.0/lib/gcc/7 -Wl,-rpath,/Library/Developer/CommandLineTools/usr/bin/../lib/clang/8.0.0/lib/darwin -L/Library/Developer/CommandLineTools/usr/bin/../lib/clang/8.0.0/lib/darwin -lpetsc -llapack -lblas -lclang_rt.osx -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lgcc_ext.10.5 -lquadmath -lm -lclang_rt.osx -lc++ -lclang_rt.osx -Wl,-rpath,/usr/local/opt/libevent/lib -L/usr/local/opt/libevent/lib -Wl,-rpath,/usr/local/Cellar/open-mpi/2.1.1/lib -L/usr/local/Cellar/open-mpi/2.1.1/lib -ldl -lmpi -lSystem -Wl,-rpath,/Library/Developer/CommandLineTools/usr/bin/../lib/clang/8.0.0/lib/darwin -L/Library/Developer/CommandLineTools/usr/bin/../lib/clang/8.0.0/lib/darwin -lclang_rt.osx -ldl  libbfs.a\n",
    "/usr/bin/dsymutil test_bfs\n",
    "warning: (x86_64) /Users/Karel/cs/cse6230/final/cse6230fa17-final-kkc3/Breadth-First-Search/parallel/libbfs.a(bfs.o) unable to open object file: No such file or directory\n",
    "/bin/rm -f -f test_bfs.o\n",
    "85505a92c5b358577baa67e9de736412340ea7de\n",
    "/Users/Karel/cs/cse6230/final/cse6230fa17-final-kkc3/Breadth-First-Search/parallel\n",
    "compette\n",
    "Thu Dec 14 03:56:53 EST 2017\n",
    "Running 1 tests of breadth first search\n",
    "  Test 0: scale 4\n",
    "    Test: 256 edges\n",
    "rank: 0\n",
    "Allreduce avg time:   0.000005\n",
    "Alltoall avg time:    0.000084\n",
    "Alltoallv 1 avg time: 0.000056\n",
    "Alltoallv 2 avg time: 0.000043\n",
    "rank: 1\n",
    "Allreduce avg time:   0.000005\n",
    "Alltoall avg time:    0.000087\n",
    "Alltoallv 1 avg time: 0.000046\n",
    "Alltoallv 2 avg time: 0.000044\n",
    "rank: 2\n",
    "Allreduce avg time:   0.000005\n",
    "Alltoall avg time:    0.000070\n",
    "Alltoallv 1 avg time: 0.000056\n",
    "Alltoallv 2 avg time: 0.000054\n",
    "rank: 3\n",
    "Allreduce avg time:   0.000004\n",
    "Alltoall avg time:    0.000084\n",
    "Alltoallv 1 avg time: 0.000048\n",
    "Alltoallv 2 avg time: 0.000049\n",
    "rank: 0\n",
    "Allreduce avg time:   0.000005\n",
    "Alltoall avg time:    0.000058\n",
    "Alltoallv 1 avg time: 0.000039\n",
    "Alltoallv 2 avg time: 0.000028\n",
    "rank: 1\n",
    "Allreduce avg time:   0.000004\n",
    "Alltoall avg time:    0.000049\n",
    "Alltoallv 1 avg time: 0.000033\n",
    "Alltoallv 2 avg time: 0.000037\n",
    "rank: 3\n",
    "Allreduce avg time:   0.000004\n",
    "Alltoall avg time:    0.000053\n",
    "Alltoallv 1 avg time: 0.000037\n",
    "Alltoallv 2 avg time: 0.000028\n",
    "rank: 2\n",
    "Allreduce avg time:   0.000004\n",
    "Alltoall avg time:    0.000049\n",
    "Alltoallv 1 avg time: 0.000033\n",
    "Alltoallv 2 avg time: 0.000039\n",
    "    Test scale 4, 16 keys: Passed, average time \n",
    "    (1 tests): 0.00913596, ***28021.1 parents per second***.\n",
    "===Harmonic mean: 28021.1 parents per second===\n",
    "Thu Dec 14 03:56:54 EST 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, there is a noticeable difference in the average time spent in the reduction, as well as variance across processes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |Mean Time Spent | Standard Deviation |\n",
    "|-|:---------------:| :------------------: |\n",
    "|`Allreduce` | 0.00003275 | 0.00000543 |\n",
    "|`Iallreduce` | 0.00000457 | 0.0000005 |\n",
    "| % decrease | 86% | 91%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The significant decrease in the average time spent in the communication suggest that the non-blocking behavior worked as hoped. Processes, on average, are spending less time waiting for others to catch up. This decrease in variance also helps validate this claim; processes are each spending roughly equal lengths of time in the routine. However, I am hesitant to claim that this evidence suffices to prove my initial proposition correct. For one thing, I would need to run these tests for a large number of checks (`num_time`) to approach a 'truer' average of these measurements. Secondly, I would require successful execution of the updated code to run on Stampede2 with a large number of checks. Despite my unsteady confidence in these results, a key takeaway is that improvements in work balance across processes could lie in switching blocking communications to non-blocking, where allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entry 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SBATCH  -J bfs                          # Job name\n",
    "#SBATCH  -p development                  # Queue (development or normal)\n",
    "#SBATCH  -N 1                            # Number of nodes\n",
    "#SBATCH --tasks-per-node 4               # Number of tasks per node\n",
    "#SBATCH  -t 00:10:00                     # Time limit hrs:min:sec\n",
    "#SBATCH  -A TG-TRA170035                 # Allocation\n",
    "#SBATCH  -o bfs=%j.out                   # Standard output and error log\n",
    "\n",
    "module use /home1/01236/tisaac/opt/modulefiles\n",
    "module load petsc/cse6230-double\n",
    "\n",
    "make test_bfs\n",
    "\n",
    "git rev-parse HEAD\n",
    "\n",
    "git diff-files\n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "#ibrun tacc_affinity ./test_bfs\n",
    "ibrun tacc_affinity hpcrun -t ./test_bfs -tests 4 -num_time 1000\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpicc -o test_bfs.o -c -fPIC  -wd1572 -g  -g -I/home1/01236/tisaac/\n",
    "opt/petsc/cse6230-double/include -I/home1/01236/tisaac/opt/petsc/cse6230-double/\n",
    "include -I/home1/01236/tisaac/opt/petsc/cse6230-double/include    `pwd`/test_bfs.c\n",
    "gmake[1]: Nothing to be done for `libc'.\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "The version of PETSc you are using is out-of-date, we recommend updating to the new release\n",
    " Available Version: 3.8.3   Installed Version: 3.8.1\n",
    "http://www.mcs.anl.gov/petsc/download/index.html\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "mpicc -fPIC  -wd1572 -g -g -o test_bfs test_bfs.o\n",
    "true test_bfs\n",
    "/bin/rm -f -f test_bfs.o\n",
    "b9977774a23d95122a69bf985382c806c0724233\n",
    "/home1/05267/tg845668/cse6230fa17-final-kkc3/Breadth-First-Search/parallel\n",
    "c455-014.stampede2.tacc.utexas.edu\n",
    "Thu Dec 14 13:08:50 CST 2017\n",
    "TACC:  Starting up job 513093\n",
    "TACC:  Starting parallel tasks...\n",
    "Running 1 tests of breadth first search\n",
    "  Test 0: scale 4\n",
    "    Test: 256 edges\n",
    "    Test scale 4, 16 keys: Passed, average time (1000 tests): 0.00479865,\n",
    "            ***53348.4 parents per second***.\n",
    "===Harmonic mean: 53348.4 parents per second===\n",
    "TACC:  Shutdown complete. Exiting.\n",
    "Thu Dec 14 13:12:16 CST 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Entry 4: hpctraceviewer](entries/entry4.png \"Entry 4: hpctraceviewer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the time spent in `BFSGraphSetEdges`, I notice that each process does not spend an equal amount of time in this graph set-up stage. When assigning edges to the different processes, I distribute the graph 1-dimensionally across the rows of the equivalent adjacency matrix so that each process owns approximately n/p vertices. This redistribution of edges according to vertex number is already an improvement in load balancing over the non-redistributing approach. However, the disparity still present could be further improved. Instead of assigning an equal sized range of vertex numbers to each process, I could instead use the total number of edges `E` to assign each process E/p edges. With this change, each process should own the same number of edges which to iterate over, thereby evening out the workload distribution. Another potential improvement would be changing the algorithm to employ a 2-D partition. Edges (u,v) would then be assigned to a mesh of pxq processes according to the ID of vertex u and of vertex v. Since vertices would be distributed across two dimensions, I would expect the average load balance to incurr less variance and the time spent in setting the edges to be more even."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
